# ğŸ“Š Sistema de AvaliaÃ§Ã£o Comparativa RAG

## ğŸ¯ Objetivo

Este sistema permite avaliar e comparar a qualidade de dois sistemas RAG de forma interativa e individualizada:
1.  **RAG Manual:** ImplementaÃ§Ã£o customizada com ChromaDB + SentenceTransformer.
2.  **File Search RAG:** SoluÃ§Ã£o gerenciada do Google Gemini.

O design atual foca no script `evaluate_light.py`, que Ã© ideal para execuÃ§Ãµes controladas, especialmente ao usar APIs com limites de requisiÃ§Ã£o (como a API gratuita do Gemini).

## ğŸ“ Arquivos do Sistema

```
01RAG/
â”œâ”€â”€ evaluate_light.py           # âœ… Script principal para avaliaÃ§Ã£o interativa
â”œâ”€â”€ file_search_rag.py          # ImplementaÃ§Ã£o do File Search RAG
â”œâ”€â”€ retriever.py                # Componentes do RAG Manual
â”œâ”€â”€ augmentation.py             # ...
â”œâ”€â”€ generation.py               # ...
â”œâ”€â”€ test_config.json            # ConfiguraÃ§Ã£o das perguntas de teste
â”œâ”€â”€ console_presenter.py        # MÃ³dulo para apresentaÃ§Ã£o no console
â”œâ”€â”€ report_generator.py         # MÃ³dulo para salvar relatÃ³rios
â”œâ”€â”€ EVALUATION_README.md        # ğŸ‘ˆ Este arquivo
â””â”€â”€ evaluation_results/         # ğŸ“‚ DiretÃ³rio onde os resultados sÃ£o salvos
```

## ğŸš€ Como Usar

Siga os passos abaixo para configurar e executar o sistema de avaliaÃ§Ã£o.

### PrÃ©-requisitos
*   Python 3.12+
*   `uv` (ou `pip`) instalado

### Passo 1. Clone o RepositÃ³rio
```bash
git clone https://github.com/matheus896/evaluate-rag.git
```

### Passo 2: Configurar VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do diretÃ³rio a partir do exemplo fornecido.

```bash
# Crie o arquivo .env
cp .env.example .env
```
Agora, edite o arquivo `.env` e adicione sua chave de API do Google Gemini:

```env
GOOGLE_API_KEY="SUA_CHAVE_DE_API_AQUI"
```

### Passo 3: Instalar as DependÃªncias

Ã‰ recomendado usar `uv` para uma instalaÃ§Ã£o mais rÃ¡pida e gerenciamento de ambiente.

```bash
# Crie um ambiente virtual
uv venv 

# Ative o ambiente
# No Windows:
.venv\Scripts\activate
# No macOS/Linux:
source .venv/bin/activate

# Instale as dependÃªncias do pyproject.toml
uv sync
```
### Passo 4. Popule as Bases de Conhecimento (ChromaDB):

Este Ã© o passo crucial de **IndexaÃ§Ã£o**. Vamos executar o script que lÃª os documentos, os divide em *chunks*, gera os *embeddings* e os armazena no ChromaDB.
```bash
# Execute o script para criar as bases de dados vetoriais
python semantic_encoder.py
```
*   Este script irÃ¡ criar uma pasta `chroma_db` e popularÃ¡ as coleÃ§Ãµes `synthetic_dataset_papers` e `direito_constitucional`, que sÃ£o usadas.

### Passo 5: Configurar Perguntas de Teste (Opcional)

VocÃª pode adicionar ou modificar as perguntas de avaliaÃ§Ã£o editando o arquivo `test_config.json`. A estrutura Ã© autoexplicativa.

### Passo 6: Executar a AvaliaÃ§Ã£o Interativa

Execute o script `evaluate_light.py` para iniciar a interface de linha de comando.

```bash
python evaluate_light.py
```

VocÃª verÃ¡ um menu de opÃ§Ãµes:

```
ğŸ“‹ OPÃ‡Ã•ES:
  [L] Listar todas as perguntas disponÃ­veis
  [1-10] Avaliar pergunta especÃ­fica (com AI Judge)
  [Q] Avaliar pergunta SEM AI Judge (apenas respostas)
  [S] Sair

ğŸ‘‰ Escolha uma opÃ§Ã£o:
```

-   **`L`**: Mostra a lista de todas as perguntas carregadas do `test_config.json`.
-   **`1-10`**: Digite o nÃºmero da pergunta para executar a avaliaÃ§Ã£o completa, incluindo a anÃ¡lise de qualidade pelo "AI Judge" (Gemini 2.5 Pro).
-   **`Q`**: Permite testar a recuperaÃ§Ã£o e geraÃ§Ã£o de respostas de ambos os sistemas RAG sem invocar o AI Judge. Ã‰ mais rÃ¡pido e nÃ£o consome chamadas extras Ã  API.
-   **`S`**: Encerra o programa.

### Passo 7: Analisar os Resultados

Para cada pergunta avaliada, o sistema gera automaticamente dois arquivos no diretÃ³rio `evaluation_results/`:

1.  **`evaluation_single_q<ID>_<timestamp>.json`**: ContÃ©m todos os dados brutos da avaliaÃ§Ã£o, incluindo respostas, chunks, latÃªncia e scores.
2.  **`evaluation_single_q<ID>_<timestamp>.md`**: Um relatÃ³rio em Markdown, formatado para fÃ¡cil leitura, com a comparaÃ§Ã£o lado a lado dos dois sistemas.

## ğŸ“‹ CritÃ©rios de AvaliaÃ§Ã£o

O sistema avalia **5 critÃ©rios** usando um "AI Judge" (Gemini 2.5 Pro):

| CritÃ©rio | Peso | DescriÃ§Ã£o |
|---|---|---|
| **ConsistÃªncia Factual** | 25% | A resposta estÃ¡ baseada no contexto? Sem alucinaÃ§Ãµes? |
| **Seguir InstruÃ§Ãµes** | 15% | A resposta segue o formato solicitado? |
| **Conhecimento do DomÃ­nio** | 20% | Usa terminologia jurÃ­dica corretamente? |
| **PrecisÃ£o do Contexto** | 20% | Os chunks recuperados sÃ£o relevantes? |
| **Cobertura do Contexto** | 20% | O contexto contÃ©m toda informaÃ§Ã£o necessÃ¡ria? |

### Escala de PontuaÃ§Ã£o (1-5)

-   **5:** Excelente
-   **4:** Muito Bom
-   **3:** AceitÃ¡vel
-   **2:** Ruim
-   **1:** Muito Ruim

## ğŸ“Š Estrutura do RelatÃ³rio em Markdown

O relatÃ³rio gerado para cada pergunta contÃ©m:
-   **Resumo da Pergunta:** Categoria e texto da pergunta.
-   **Resultados Individuais:** Para cada sistema (RAG Manual e File Search RAG), sÃ£o apresentados:
    -   MÃ©tricas de latÃªncia e nÃºmero de chunks.
    -   A resposta gerada.
    -   A tabela de avaliaÃ§Ã£o do AI Judge com scores e justificativas.
-   **ComparaÃ§Ã£o de Scores:** Uma tabela consolidada comparando as pontuaÃ§Ãµes de ambos os sistemas para cada critÃ©rio.
-   **AnÃ¡lise de Chunks:** Uma anÃ¡lise comparativa do material recuperado (nÃºmero de chunks, total de caracteres, etc.).

<img src="evaluate-ai-judge.png" alt="Exemplo de RelatÃ³rio em Markdown" width="600"/>

## ğŸ› Troubleshooting

### Erro: "Store nÃ£o encontrado" (File Search RAG)
O `file_search_rag.py` tenta criar o *store* automaticamente. Se falhar, verifique suas permissÃµes de API.

### Erro: "ChromaDB collection nÃ£o encontrada" (RAG Manual)
Certifique-se de que a coleÃ§Ã£o foi criada. Se necessÃ¡rio, execute o script `semantic_encoder.py` novamente para popular o ChromaDB.

### Erro de Rate Limit (503 Service Unavailable)
O `evaluate_light.py` jÃ¡ possui um sistema de *retry* com *exponential backoff*. Se o erro persistir, aguarde alguns minutos antes de tentar novamente. A API gratuita tem limites agressivos.