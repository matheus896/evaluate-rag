# ğŸ“Š Sistema de AvaliaÃ§Ã£o Comparativa RAG

## ğŸ¯ Objetivo

Este sistema permite avaliar e comparar a qualidade de dois sistemas RAG de forma interativa e individualizada:
1.  **RAG Manual:** ImplementaÃ§Ã£o customizada com ChromaDB + SentenceTransformer.
2.  **File Search RAG:** SoluÃ§Ã£o gerenciada do Google Gemini.

O design atual foca no script `evaluate_light.py`, que Ã© ideal para execuÃ§Ãµes controladas, especialmente ao usar APIs com limites de requisiÃ§Ã£o (como a API gratuita do Gemini).

## ğŸ“ Arquivos do Sistema

```
01RAG/
â”œâ”€â”€ evaluate_light.py           # âœ… Script principal para avaliaÃ§Ã£o interativa
â”œâ”€â”€ ai_judge.py                 # âœ… NOVO: MÃ³dulo AI Judge com LiteLLM (agnÃ³stico)
â”œâ”€â”€ test_litellm_providers.py   # âœ… NOVO: Teste isolado de provedores
â”œâ”€â”€ file_search_rag.py          # ImplementaÃ§Ã£o do File Search RAG
â”œâ”€â”€ retriever.py                # Componentes do RAG Manual
â”œâ”€â”€ augmentation.py             # ...
â”œâ”€â”€ generation.py               # ...
â”œâ”€â”€ test_config.json            # ConfiguraÃ§Ã£o das perguntas de teste
â”œâ”€â”€ console_presenter.py        # MÃ³dulo para apresentaÃ§Ã£o no console
â”œâ”€â”€ report_generator.py         # MÃ³dulo para salvar relatÃ³rios
â”œâ”€â”€ .env.example                # Template de variÃ¡veis de ambiente
â”œâ”€â”€ README.md                   # Este arquivo
â””â”€â”€ evaluation_results/         # ğŸ“‚ DiretÃ³rio onde os resultados sÃ£o salvos
```

## ğŸš€ Como Usar

Siga os passos abaixo para configurar e executar o sistema de avaliaÃ§Ã£o.

### PrÃ©-requisitos
*   Python 3.12+
*   `uv` (ou `pip`) instalado

### Passo 1. Clone o RepositÃ³rio
```bash
git clone https://github.com/matheus896/evaluate-rag.git
```

### Passo 2: Configurar VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do diretÃ³rio a partir do exemplo fornecido.

```bash
# Crie o arquivo .env
cp .env.example .env
```
Agora, edite o arquivo `.env` e adicione sua chave de API do Google Gemini:

```env
GOOGLE_API_KEY="SUA_CHAVE_DE_API_AQUI"
```

### Passo 3: Instalar as DependÃªncias

Ã‰ recomendado usar `uv` para uma instalaÃ§Ã£o mais rÃ¡pida e gerenciamento de ambiente.

```bash
# Crie um ambiente virtual
uv venv 

# Ative o ambiente
# No Windows:
.venv\Scripts\activate
# No macOS/Linux:
source .venv/bin/activate

# Instale as dependÃªncias do pyproject.toml
uv sync
```
### Passo 4. Popule as Bases de Conhecimento (ChromaDB):

Este Ã© o passo crucial de **IndexaÃ§Ã£o**. Vamos executar o script que lÃª os documentos, os divide em *chunks*, gera os *embeddings* e os armazena no ChromaDB.
```bash
# Execute o script para criar as bases de dados vetoriais
python semantic_encoder.py
```
*   Este script irÃ¡ criar uma pasta `chroma_db` e popularÃ¡ as coleÃ§Ãµes `synthetic_dataset_papers` e `direito_constitucional`, que sÃ£o usadas.

### Passo 5: Configurar Perguntas de Teste (Opcional)

VocÃª pode adicionar ou modificar as perguntas de avaliaÃ§Ã£o editando o arquivo `test_config.json`. A estrutura Ã© autoexplicativa.

### Passo 6: Executar a AvaliaÃ§Ã£o Interativa

Execute o script `evaluate_light.py` para iniciar a interface de linha de comando.

```bash
python evaluate_light.py
```

VocÃª verÃ¡ um menu de opÃ§Ãµes:

```
ğŸ“‹ OPÃ‡Ã•ES:
  [L] Listar todas as perguntas disponÃ­veis
  [1-10] Avaliar pergunta especÃ­fica (com AI Judge)
  [Q] Avaliar pergunta SEM AI Judge (apenas respostas)
  [S] Sair

ğŸ‘‰ Escolha uma opÃ§Ã£o:
```

-   **`L`**: Mostra a lista de todas as perguntas carregadas do `test_config.json`.
-   **`1-10`**: Digite o nÃºmero da pergunta para executar a avaliaÃ§Ã£o completa, incluindo a anÃ¡lise de qualidade pelo "AI Judge" (configurÃ¡vel via LiteLLM - padrÃ£o: Cerebras).
-   **`Q`**: Permite testar a recuperaÃ§Ã£o e geraÃ§Ã£o de respostas de ambos os sistemas RAG sem invocar o AI Judge. Ã‰ mais rÃ¡pido e nÃ£o consome chamadas extras Ã  API.
-   **`S`**: Encerra o programa.

### Passo 7: Analisar os Resultados

Para cada pergunta avaliada, o sistema gera automaticamente dois arquivos no diretÃ³rio `evaluation_results/`:

1.  **`evaluation_single_q<ID>_<timestamp>.json`**: ContÃ©m todos os dados brutos da avaliaÃ§Ã£o, incluindo respostas, chunks, latÃªncia e scores.
2.  **`evaluation_single_q<ID>_<timestamp>.md`**: Um relatÃ³rio em Markdown, formatado para fÃ¡cil leitura, com a comparaÃ§Ã£o lado a lado dos dois sistemas.

## ğŸ”„ Usando LiteLLM para Alterar o AI Judge

A partir da versÃ£o atual, o sistema de avaliaÃ§Ã£o utiliza **LiteLLM** para desacoplar o AI Judge de um Ãºnico provedor. Isso significa que vocÃª pode facilmente trocar entre diferentes provedores de LLM (Cerebras, OpenAI, Anthropic, etc.) sem alterar o cÃ³digo principal.

### Passo 1: Testar o Provedor Isoladamente

Antes de integrar um novo provedor ao sistema de avaliaÃ§Ã£o, teste-o de forma isolada usando o script `test_litellm_providers.py`:

```bash
python test_litellm_providers.py
```

### Passo 2: Configurar o Provedor (test_litellm_providers.py)

Edite o arquivo `test_litellm_providers.py` e altere as linhas de configuraÃ§Ã£o do provedor:

**Linha 44** - Altere o `"model"` para o modelo de sua escolha:
```python
"model": "cerebras/gpt-oss-120b",  # Exemplo com Cerebras
```

Para encontrar o formato correto do seu modelo desejado, consulte a documentaÃ§Ã£o oficial:
ğŸ‘‰ **https://docs.litellm.ai/docs/providers**

Exemplos de formatos vÃ¡lidos:
- **Cerebras**: `"cerebras/gpt-oss-120b"` ou `"cerebras/llama-3.3-70b"`
- **OpenAI**: `"openai/gpt-4o"` ou `"openai/gpt-4-turbo"`
- **Anthropic**: `"anthropic/claude-3-sonnet-20240229"`
- **Google Gemini**: `"vertex_ai/gemini-2.5-pro"`

**Linha 45** - Altere a `"api_key_env"` para a variÃ¡vel de ambiente que vocÃª definiu no seu `.env`:
```python
"api_key_env": "CEREBRAS_API_KEY",  # Exemplo para Cerebras
```

Se vocÃª definiu no seu `.env`:
```env
# Exemplos
CEREBRAS_API_KEY="sua-chave-aqui"
OPENAI_API_KEY="sua-chave-aqui"
ANTHROPIC_API_KEY="sua-chave-aqui"
GOOGLE_API_KEY="sua-chave-aqui"
```

### Passo 3: Executar o Teste

```bash
python test_litellm_providers.py
```

### O Que o Teste Mostra

O script `test_litellm_providers.py` executa trÃªs validaÃ§Ãµes para seu provedor:

| Teste | O Que Valida | Esperado |
|-------|-------------|----------|
| **Teste BÃ¡sico** | Conectividade e resposta simples | âœ… Resposta "OK" em poucos segundos |
| **Teste JSON Mode** | Resposta estruturada em JSON (crÃ­tico para o AI Judge) | âœ… JSON vÃ¡lido parseado corretamente |
| **Teste Streaming** | Streaming de respostas (opcional, nÃ£o bloqueante) | âœ… Chunks recebidos sequencialmente |

**Exemplo de saÃ­da bem-sucedida:**
```
================================================================================
ğŸ“Š RESUMO DE TESTES: PROVIDER
================================================================================

âœ… Passaram: 2/2
âŒ Falharam: 0/2
âŠ˜ Pulados: 0/2

âœ… BASIC: success
âœ… JSON_MODE: success

âœ… TODOS OS TESTES PASSARAM!

ğŸ‰ O provedor estÃ¡ pronto para integraÃ§Ã£o com o AI Judge.
```

### Passo 4: Configurar no test_config.json

ApÃ³s o teste passar com sucesso, edite o arquivo `test_config.json` e altere a linha do `ai_judge_model`:

**Linha 88** - Altere para o seu provedor:
```json
"ai_judge_model": "cerebras/gpt-oss-120b",
"ai_judge_temperature": 0.1
```

### Passo 5: Executar a AvaliaÃ§Ã£o com o Novo Provedor

```bash
python evaluate_light.py
```

Escolha uma pergunta para avaliar (por exemplo, digite `1` para avaliar a pergunta 1).

O sistema agora usarÃ¡ seu provedor configurado via LiteLLM para realizar as avaliaÃ§Ãµes do AI Judge.

### Exemplo Completo: Migrando para OpenAI

Se vocÃª quiser usar OpenAI em vez de Cerebras, siga este exemplo:

**1. Configure seu `.env`:**
```env
GOOGLE_API_KEY="sua-chave-gemini"
OPENAI_API_KEY="sk-sua-chave-openai"
```

**2. Edite `test_litellm_providers.py` (linhas 44-45):**
```python
"model": "openai/gpt-4o",
"api_key_env": "OPENAI_API_KEY",
```

**3. Execute o teste:**
```bash
python test_litellm_providers.py
```

**4. Se passar, edite `test_config.json` (linha 88):**
```json
"ai_judge_model": "openai/gpt-4o",
```

**5. Execute a avaliaÃ§Ã£o:**
```bash
python evaluate_light.py
```

---

## ğŸ“‹ CritÃ©rios de AvaliaÃ§Ã£o

O sistema avalia **5 critÃ©rios** usando um "AI Judge" (configurÃ¡vel via LiteLLM):

| CritÃ©rio | Peso | DescriÃ§Ã£o |
|---|---|---|
| **ConsistÃªncia Factual** | 25% | A resposta estÃ¡ baseada no contexto? Sem alucinaÃ§Ãµes? |
| **Seguir InstruÃ§Ãµes** | 15% | A resposta segue o formato solicitado? |
| **Conhecimento do DomÃ­nio** | 20% | Usa terminologia jurÃ­dica corretamente? |
| **PrecisÃ£o do Contexto** | 20% | Os chunks recuperados sÃ£o relevantes? |
| **Cobertura do Contexto** | 20% | O contexto contÃ©m toda informaÃ§Ã£o necessÃ¡ria? |

### Escala de PontuaÃ§Ã£o (1-5)

-   **5:** Excelente
-   **4:** Muito Bom
-   **3:** AceitÃ¡vel
-   **2:** Ruim
-   **1:** Muito Ruim

## ğŸ“Š Estrutura do RelatÃ³rio em Markdown

O relatÃ³rio gerado para cada pergunta contÃ©m:
-   **Resumo da Pergunta:** Categoria e texto da pergunta.
-   **Resultados Individuais:** Para cada sistema (RAG Manual e File Search RAG), sÃ£o apresentados:
    -   MÃ©tricas de latÃªncia e nÃºmero de chunks.
    -   A resposta gerada.
    -   A tabela de avaliaÃ§Ã£o do AI Judge com scores e justificativas.
-   **ComparaÃ§Ã£o de Scores:** Uma tabela consolidada comparando as pontuaÃ§Ãµes de ambos os sistemas para cada critÃ©rio.
-   **AnÃ¡lise de Chunks:** Uma anÃ¡lise comparativa do material recuperado (nÃºmero de chunks, total de caracteres, etc.).

<img src="evaluate-ai-judge.png" alt="Exemplo de RelatÃ³rio em Markdown" width="600"/>

## ğŸ› Troubleshooting

### Erro: "Store nÃ£o encontrado" (File Search RAG)
O `file_search_rag.py` tenta criar o *store* automaticamente. Se falhar, verifique suas permissÃµes de API.

### Erro: "ChromaDB collection nÃ£o encontrada" (RAG Manual)
Certifique-se de que a coleÃ§Ã£o foi criada. Se necessÃ¡rio, execute o script `semantic_encoder.py` novamente para popular o ChromaDB.

### Erro de Rate Limit (503 Service Unavailable)
O `evaluate_light.py` jÃ¡ possui um sistema de *retry* com *exponential backoff*. Se o erro persistir, aguarde alguns minutos antes de tentar novamente. A API gratuita tem limites agressivos.